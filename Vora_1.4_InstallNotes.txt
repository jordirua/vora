##################################
# SAP HANA VORA 1.4 INSTALLATION #
##################################

# AWS
launch suse-sles-11-sp4-sapcal-v20160415-hvm-ssd-x86_64 AMI in AWS 
Make HD 40 GB minimum

# when logged in
sudo su -
zypper ref -s
zypper update -t patch
zypper update -t patch
/etc/init.d/ntp restart

# prepare dlog (see install PDF for other services (not needed on my AWS image))
zypper install libaio
cat /proc/sys/fs/file-max
echo "fs.file-max=983040" | tee -a /etc/sysctl.conf
sysctl -p
cat /etc/security/limits.conf
echo "*                -       nofile          1000000" | tee -a /etc/security/limits.conf
cat /etc/security/limits.conf
locale
export LC_ALL=en_US.UTF-8
locale
exit
sudo su -

chmod 700 /etc/sudoers
echo "%sysadmin ALL=(ALL) NOPASSWD:ALL" | tee -a /etc/sudoers
chmod 440 /etc/sudoers
groupadd sysadmin
/usr/sbin/useradd -m -g users -G sysadmin cluster_admin
su - cluster_admin

ssh-keygen -t rsa
PRESS RETURN
PRESS RETURN

chmod 700 ~/
chmod 700 ~/.ssh
cat ~/.ssh/id_rsa.pub >~/.ssh/authorized_keys
chmod 600 ~/.ssh/authorized_keys
cat ~/.ssh/id_rsa.pub
cat ~/.ssh/id_rsa

# +++++++++++++ #
#   AMBARI      #
# +++++++++++++ #

cd /etc/zypp/repos.d
sudo wget http://public-repo-1.hortonworks.com/ambari/suse11/2.x/updates/2.2.2.0/ambari.repo -O /etc/zypp/repos.d/ambari.repo
sudo zypper ref
sudo zypper install ambari-server

sudo /usr/sbin/ambari-server setup
ACCEPT DEFAULTS
sudo /usr/sbin/ambari-server restart

-- Install HDP 2.4 with repo 2.4.2
-- Create Cluster; When selecting key, use the bottom Private one 
-- use "cluster_admin" user
-- Install HDFS, Yarn, Spark, Ambari Metrics, Hive (needed because of Spark), No Zookeeper
-- assign EVERYTHING on all slaves and clients

+++++++++++++++
+ HIVE CONFIG +
+++++++++++++++

# when prompted for hive login and password for hive repo, need to do following steps
# should be root
exit 
zypper install -y postgresql-jdbc
ls /usr/share/java/postgresql-jdbc.jar
chmod 644 /usr/share/java/postgresql-jdbc.jar
ambari-server setup --jdbc-db=postgres --jdbc-driver=/usr/share/java/postgresql-jdbc.jar

su - postgres
psql
create database hive;
create user hive with password 'hive';
grant all privileges on database hive to hive;
\q
# should be root now
exit 
cp /var/lib/pgsql/data/pg_hba.conf /var/lib/pgsql/data/pg_hba.conf_backup
vi /var/lib/pgsql/data/pg_hba.conf
-- add hive to the list of users at the bottom (so hive,mapred,ambari)
:wq!
sudo /etc/init.d/postgresql restart

+++++++++++++++
+  TEST HDFS  +
+++++++++++++++

exit
su - hdfs
hdfs dfs -mkdir /user/cluster_admin
hdfs dfs -chown cluster_admin /user/cluster_admin
echo "1,2,Hello" > test.csv
hdfs dfs -ls /user/cluster_admin
hdfs dfs -put test.csv /user/cluster_admin
hdfs dfs -ls /user/cluster_admin
hdfs dfs -cat /user/cluster_admin/test.csv
exit

++++++++++++++++
+ SETUP SPARK  +
++++++++++++++++

sudo su -
sudo vi /etc/bash.bashrc
-- Type G, I (in VI)
Paste ;
## Paths in bash.bashrc FOR AMBARI ##
export JAVA_HOME=/usr/jdk64/jdk1.8.0_60/
export HADOOP_CONF_DIR=/etc/hadoop/conf
export SPARK_HOME=/usr/hdp/2.4.3.0-227/spark/
export SPARK_CONF_DIR=$SPARK_HOME/conf
export PATH=$PATH:$SPARK_HOME/bin
exit
sudo su -
su - cluster_admin

++++++++++++++++
+  TEST SPARK 
++++++++++++++++

spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client --num-executors 2 --driver-memory 512m --executor-memory 512m --executor-cores 2 --queue default $SPARK_HOME/lib/spark-examples*.jar 10 2>/dev/null

# should see "Pi is roughly 3.140292"

++++++++++++++++
+ VORA INSTALL +
++++++++++++++++

id
# should be root user
rm SAPHanaVora*.gz
sudo rm -r /tmp/SAPHanaVora/
wget ?????????????????????????? -O SAPHanaVora-ambari.tar.gz
sudo tar -xvzf SAPHanaVora-ambari.tar.gz -C /tmp
cd /tmp/SAPHanaVora/
ls
./install.sh --install-dep

ls /var/lib/ambari-server/resources/stacks/HDP/2.4/services

-- make sure all services started
-- add vora manager service in ambari 
-- for vora manager, if a single node, add vora master, client & worker
-- for vora config
ls /usr/hdp/2.4.3.0-227/spark
ls /usr/jdk64/jdk1.8.0_60

###########
# AWS Fix #
###########

-- mention that below is a fix for AWS, but will be fixed
-- mismatch of address (in aws different internal and external address). 

sudo touch /etc/vora/aws
ls /var/lib/ambari-agent/cache/stacks/HDP/2.4/services/VORA/package/scripts/
-- Open params.py file: 
sudo vi /var/lib/ambari-agent/cache/stacks/HDP/2.4/services/VORA/package/scripts/params.py

Add: 
import socket
#self_host = config['public_hostname']
self_host = socket.getfqdn()
 
Then save the above file and restart Vora via Ambari.

++++++++++++++++
+ TO TEST VORA +
++++++++++++++++

tail -f /var/log/vora-manager
http://vora14:19000/
http://vora14:9225/
vora/????

cat /etc/vora/vora-env.sh

cd /opt/vora/lib/vora-spark

sudo vi /etc/bash.bashrc
-- Type G, I (in VI)
Paste ;
## Paths in bash.bashrc FOR AMBARI ##
export VORA_SPARK_HOME=/opt/vora/lib/vora-spark
-- exit and go back in as cluster_admin

ls $VORA_SPARK_HOME
ls $VORA_SPARK_HOME/bin/
$VORA_SPARK_HOME/bin/start-spark-shell.sh

import org.apache.spark.sql.SapSQLContext

val vc = new SapSQLContext(sc)

vc.sql("show tables").show

val testsql = 
"""
create table testtable (a1 int, a2 int, a3 string)
using com.sap.spark.vora
options (files "/user/cluster_admin/test.csv")
"""

vc.sql(testsql).show

vc.sql("show tables").show

vc.sql("select * from testtable").show

vc.sql("drop table testtable").show

vc.sql("show tables").show

++++++++++++++++++++
+ ZEPPELIN INSTALL +
++++++++++++++++++++

# to find scala version 2.10 uses zep 0.6.0
util.Properties.versionString

http://zeppelin.apache.org/download.html

wget http://archive.apache.org/dist/zeppelin/zeppelin-0.6.0/zeppelin-0.6.0-bin-all.tgz -O zeppelin-0.6.0-bin-all.tgz

tar -xvzf zeppelin-0.6.0-bin-all.tgz

sudo vi /etc/bash.bashrc
-- Type G, I (in VI)
Paste ;
export ZEPPELIN_HOME=/home/cluster_admin/zeppelin-0.6.0-bin-all/
exit
su - cluster_admin

ls $VORA_SPARK_HOME/zeppelin/

ls $ZEPPELIN_HOME/interpreter/spark/

cp $VORA_SPARK_HOME/zeppelin/zeppelin-*.jar $ZEPPELIN_HOME/interpreter/spark/

cd $ZEPPELIN_HOME/interpreter/spark/
jar xf zeppelin-1*.jar interpreter-setting.json
ls
jar uf zeppelin-spark-*.jar interpreter-setting.json
rm interpreter-setting.json

cd $ZEPPELIN_HOME/conf/
cp zeppelin-env.sh.template zeppelin-env.sh
chmod 0755 $ZEPPELIN_HOME/conf/zeppelin-env.sh
sudo vi $ZEPPELIN_HOME/conf/zeppelin-env.sh
export MASTER=yarn-client
export HADOOP_CONF_DIR=/etc/hadoop/conf
export SPARK_HOME=/usr/hdp/2.4.3.0-227/spark

cp $ZEPPELIN_HOME/conf/zeppelin-site.xml.template $ZEPPELIN_HOME/conf/zeppelin-site.xml

chmod 0755 $ZEPPELIN_HOME/conf/zeppelin-site.xml

vi $ZEPPELIN_HOME/conf/zeppelin-site.xml
- / to search
- <name>zeppelin.interpreters</name>
- add <value>,sap.zeppelin.spark.SapSqlInterpreter</value>
- add above interpreter after 1st spark one
- / to search
- <name>zeppelin.server.port</name>
- <value>from 8080 to 9099</value>
- <description>Server port.</description>

ls /usr/hdp/2.4.3.0-227/
In Ambari, select YARN service, choose /Configs/Advanced/Custom yarn-site.
Choose Add Property "hdp.version" and make value "2.4.3.0-227".

$ZEPPELIN_HOME/bin/zeppelin-daemon.sh restart

http://vora14:9099

cd $VORA_SPARK_HOME/lib
ls
pwd

# paste into dependency
/opt/vora/lib/vora-spark/lib/spark-sap-datasources-1.4.2.12-vora-1.4-assembly.jar

# make sure "master" is set to yarn-client

%spark.vora show tables

%spark.vora create table testtable (a1 int, a2 int, a3 string)
using com.sap.spark.vora
options (files "/user/cluster_admin/test.csv")

%spark.vora show tables

%spark.vora select * from testtable

+++++++++++++++
+ DELETE VORA +
+++++++++++++++

curl -u admin:admin -X DELETE -H 'X-Requested-By:admin' http://localhost:8080/api/v1/clusters/SHA/services/HANA_VORA_MANAGER

sudo rm -rf /var/lib/ambari-server/resources/stacks/HDP/2.4/services/VORA*
sudo rm -rf /var/lib/ambari-agent/cache/stacks/HDP/2.4/services/VORA*
sudo rm -rf /var/log/vora-manager
sudo rm -rf /var/log/vora*
sudo rm -rf /etc/vora/
sudo rm -rf /var/local/vora*
sudo rm -rf /var/run/vora*
sudo rm -rf /lib/vora*
sudo rm -rf /run/lock/vora/
sudo rm -rf /var/lock/vora/
sudo rm -rf /var/log/messages-*

sudo /usr/sbin/ambari-agent restart
sudo /usr/sbin/ambari-server restart
